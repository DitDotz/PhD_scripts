{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Notebook demonstrates novel unsupervised ML workflow that works on concatenated 4D-STEM and STEM-EDS datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "import hyperspy.api as hs\n",
    "import pyxem as pxm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palette_for_label_map(no_cluster, unclassified=False, mask=False):\n",
    "    \"\"\"\n",
    "    no_cluster = number of real clusters\n",
    "    unclassified = no_cluster + 1\n",
    "    mask = no_cluster + 2\n",
    "\n",
    "    Returns palette, CustomCmap\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib\n",
    "\n",
    "    custom_palette = [plt.cm.tab20(i) for i in range(no_cluster)]\n",
    "\n",
    "    if unclassified == True:\n",
    "        custom_palette.insert(0, \"gray\")  # UNCLASSIFIED\n",
    "    if mask == True:\n",
    "        custom_palette.insert(0, \"black\")  # MASKED OUT\n",
    "\n",
    "    CustomCmap = matplotlib.colors.ListedColormap(custom_palette)\n",
    "    palette = sns.color_palette(palette=custom_palette)\n",
    "    return (palette, CustomCmap)\n",
    "\n",
    "def include_scalebar(dp):\n",
    "    from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "    dx = dp.axes_manager[0].scale\n",
    "    scalebar = ScaleBar(\n",
    "        dx,\n",
    "        \"nm\",\n",
    "        length_fraction=0.25,\n",
    "        width_fraction=0.015,\n",
    "        location=\"lower left\",\n",
    "        frameon=False,\n",
    "        color=\"w\",\n",
    "        scale_loc=\"top\",\n",
    "        border_pad=0.5,\n",
    "    )\n",
    "    plt.gca().add_artist(scalebar)\n",
    "\n",
    "\n",
    "def plot_label_map(labels_highest_soft_spatial, no_cluster_soft, scalebar=False):\n",
    "    palette, CustomCmap = palette_for_label_map(\n",
    "        no_cluster_soft, unclassified=False, mask=False\n",
    "    )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(labels_highest_soft_spatial, cmap=CustomCmap)\n",
    "\n",
    "    if scalebar == True:\n",
    "        include_scalebar(dp)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_manifold_labels(no_cluster_soft,labels_highest_soft, reduced_data ):\n",
    "\n",
    "    import seaborn as sns\n",
    "    cluster_colors = [palette_for_label_map(no_cluster_soft)[0][x] if x >= 0\n",
    "                    else (0.5, 0.5, 0.5)\n",
    "                    for x in labels_highest_soft]\n",
    "    if reduced_data.shape[1]==3:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], alpha=0.2,  c=cluster_colors)\n",
    "    \n",
    "    if reduced_data.shape[1]==2:\n",
    "        fig = plt.figure()\n",
    "        plt.scatter(reduced_data[:,0], reduced_data[:,1], alpha=0.2,  c=cluster_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_cluster_std_dev(selected_dp, center_x=128, center_y=128, radius=11):\n",
    "    cluster_std_dev_dp_pixels = np.std(selected_dp, axis=0)\n",
    "    x, y = np.meshgrid(\n",
    "        np.arange(cluster_std_dev_dp_pixels.data.shape[0]),\n",
    "        np.arange(cluster_std_dev_dp_pixels.data.shape[1]),\n",
    "    )\n",
    "    distances = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)\n",
    "    mask = distances <= radius\n",
    "\n",
    "    cluster_std_dev_dp_pixels_masked = np.copy(cluster_std_dev_dp_pixels)\n",
    "    cluster_std_dev_dp_pixels_masked[mask] = 0\n",
    "    return cluster_std_dev_dp_pixels_masked\n",
    "\n",
    "\n",
    "def get_cluster_mean_dp_list(dp, memberships_highest_soft, labels_highest_soft):\n",
    "    dp.fold()\n",
    "    dp.unfold_navigation_space()\n",
    "    dp_soft_cluster_mean_list = []\n",
    "\n",
    "    for i in range(no_cluster_soft):\n",
    "        selected_dp = np.take(\n",
    "            dp.data,\n",
    "            np.where((memberships_highest_soft == 1) & (labels_highest_soft == i))[0],\n",
    "            axis=0,\n",
    "        )\n",
    "        dp_soft_cluster_mean = np.mean(selected_dp, axis=0)\n",
    "        dp_soft_cluster_mean_list.append(dp_soft_cluster_mean)\n",
    "    return dp_soft_cluster_mean_list\n",
    "\n",
    "\n",
    "def get_cluster_std_dev_dp_list(dp, memberships_highest_soft, labels_highest_soft):\n",
    "    dp.fold()\n",
    "    dp.unfold_navigation_space()\n",
    "    dp_soft_cluster_std_dev_list = []\n",
    "    for i in range(no_cluster_soft):\n",
    "        selected_dp = np.take(\n",
    "            dp.data,\n",
    "            np.where((memberships_highest_soft == 1) & (labels_highest_soft == i))[0],\n",
    "            axis=0,\n",
    "        )\n",
    "        dp_soft_cluster_std_dev_list.append(dp_cluster_std_dev(selected_dp))\n",
    "    return dp_soft_cluster_std_dev_list\n",
    "\n",
    "\n",
    "def get_cluster_mean_eds_list(eds, memberships_highest_soft, labels_highest_soft):\n",
    "    eds.fold()\n",
    "    eds.unfold_navigation_space()\n",
    "    eds_soft_cluster_mean_list = []\n",
    "    for i in range(no_cluster_soft):\n",
    "        selected_eds = np.take(\n",
    "            eds.data,\n",
    "            np.where((memberships_highest_soft == 1) & (labels_highest_soft == i))[0],\n",
    "            axis=0,\n",
    "        )\n",
    "        eds_soft_cluster_mean = np.mean(selected_eds, axis=0)\n",
    "        eds_soft_cluster_mean_list.append(eds_soft_cluster_mean)\n",
    "    return eds_soft_cluster_mean_list\n",
    "\n",
    "\n",
    "def get_cluster_std_dev_eds_list(eds, memberships_highest_soft, labels_highest_soft):\n",
    "    eds.fold()\n",
    "    eds.unfold_navigation_space()\n",
    "    eds_soft_cluster_std_dev_list = []\n",
    "    for i in range(no_cluster_soft):\n",
    "        selected_eds = np.take(\n",
    "            eds.data,\n",
    "            np.where((memberships_highest_soft == 1) & (labels_highest_soft == i))[0],\n",
    "            axis=0,\n",
    "        )\n",
    "        eds_soft_cluster_std_dev = np.std(selected_eds, axis=0)\n",
    "        eds_soft_cluster_std_dev_list.append(eds_soft_cluster_std_dev)\n",
    "    return eds_soft_cluster_std_dev_list\n",
    "\n",
    "\n",
    "def plot_cluster_mean_analysis(\n",
    "    include_eds_mean_plot=False,\n",
    "    include_eds_std_dev_plot=False,\n",
    "    include_dp_std_dev_plot=False,\n",
    "    include_dp_mean_plot=True,\n",
    "    include_probability_map=True,\n",
    "    dp_vmax=0.1,\n",
    "):\n",
    "    from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "    palette, CustomCmap = palette_for_label_map(\n",
    "        no_cluster_soft, unclassified=False, mask=False\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of rows required based on the plots to be included\n",
    "    total_rows = 0\n",
    "\n",
    "    if include_dp_mean_plot:\n",
    "        total_rows += 1\n",
    "\n",
    "    if include_probability_map:\n",
    "        total_rows += 2\n",
    "\n",
    "    if include_dp_std_dev_plot:\n",
    "        total_rows += 1\n",
    "\n",
    "    if include_eds_mean_plot:\n",
    "        total_rows += 1\n",
    "\n",
    "    if include_eds_std_dev_plot:\n",
    "        total_rows += 1\n",
    "\n",
    "    fig = plt.figure(figsize=(7.08661, 7.08661))\n",
    "\n",
    "    gs = plt.GridSpec(\n",
    "        total_rows,\n",
    "        no_cluster_soft,\n",
    "        wspace=0.1,\n",
    "        hspace=0.1,\n",
    "        height_ratios=[1] * (total_rows - 1) + [0.1],\n",
    "    )\n",
    "\n",
    "    for i in range(no_cluster_soft):\n",
    "        row_index = 0  # Initialize row index\n",
    "\n",
    "        ax_outer = plt.subplot(gs[:, i])\n",
    "        ax_outer.set_facecolor(palette[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        if include_dp_mean_plot:\n",
    "            dp_soft_cluster_mean_list = get_cluster_mean_dp_list(\n",
    "                dp, memberships_highest_soft, labels_highest_soft\n",
    "            )\n",
    "\n",
    "            ax_ebsd = fig.add_subplot(gs[row_index, i])\n",
    "            dp_cluster_mean = dp_soft_cluster_mean_list[i]  # current cluster\n",
    "            dp_cluster_mean_normalize = (dp_cluster_mean - dp_cluster_mean.min()) / (\n",
    "                dp_cluster_mean.max() - dp_cluster_mean.min()\n",
    "            )  # normalize\n",
    "            plt.imshow(dp_cluster_mean_normalize, cmap=\"Greys_r\", vmax=dp_vmax)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            row_index += 1\n",
    "\n",
    "        if include_dp_std_dev_plot:\n",
    "            dp_soft_cluster_std_dev_list = get_cluster_std_dev_dp_list(\n",
    "                dp, memberships_highest_soft, labels_highest_soft\n",
    "            )\n",
    "            ax_dp_std = fig.add_subplot(gs[row_index, i])\n",
    "            dp_cluster_std = dp_soft_cluster_std_dev_list[i]\n",
    "            plt.imshow(dp_cluster_std, cmap=\"inferno\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            row_index += 1\n",
    "\n",
    "        if include_eds_mean_plot:\n",
    "            eds_soft_cluster_mean_list = get_cluster_mean_eds_list(\n",
    "                eds, memberships_highest_soft, labels_highest_soft\n",
    "            )\n",
    "            ax_eds = fig.add_subplot(gs[row_index, i])\n",
    "            eds_cluster_mean = eds_soft_cluster_mean_list[i]\n",
    "            plt.plot(eds_cluster_mean)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            row_index += 1\n",
    "\n",
    "        if include_eds_std_dev_plot:\n",
    "            eds_soft_cluster_std_dev_list = get_cluster_std_dev_eds_list(\n",
    "                eds, memberships_highest_soft, labels_highest_soft\n",
    "            )\n",
    "            ax_eds_std = fig.add_subplot(gs[row_index, i])\n",
    "            eds_cluster_std = eds_soft_cluster_std_dev_list[i]\n",
    "            plt.plot(eds_cluster_std, \"g\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            row_index += 1\n",
    "\n",
    "        if include_probability_map:\n",
    "            ax_loading = fig.add_subplot(gs[row_index, i])\n",
    "            memberships_highest_soft_spatial2 = memberships_highest_soft_spatial.copy()\n",
    "            memberships_highest_soft_spatial2[\n",
    "                np.where(labels_highest_soft_spatial != i)\n",
    "            ] = 0\n",
    "            pc = plt.imshow(memberships_highest_soft_spatial2)\n",
    "\n",
    "            # scalebar = ScaleBar(2, 'nm', length_fraction=0.25, width_fraction=0.015, location='lower left',\n",
    "            #             frameon=False, color='w', scale_loc='top', border_pad=0.1)  # 1 pixel = 0.2 meter\n",
    "            # plt.gca().add_artist(scalebar)\n",
    "\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            row_index += 1\n",
    "\n",
    "    ax_colorbar = fig.add_subplot(gs[row_index, :])\n",
    "    plt.colorbar(pc, cax=ax_colorbar, orientation=\"horizontal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data using hyperspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = f\"C:\\\\Users\\\\Zhi Quan\\\\Dropbox (Personal)\\\\Jupyter backup\\\\Heusler alloy\\\\Data\\\\06_15_22\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\datascience_3.9\\lib\\site-packages\\hyperspy\\io.py:650: VisibleDeprecationWarning: Loading old file version. The binned attribute has been moved from metadata.Signal to axis.is_binned. Setting this attribute for all signal axes instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Miniconda3\\envs\\datascience_3.9\\lib\\site-packages\\hyperspy\\io.py:650: VisibleDeprecationWarning: Loading old file version. The binned attribute has been moved from metadata.Signal to axis.is_binned. Setting this attribute for all signal axes instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signal2D, title: , dimensions: (150, 50|256, 256)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eds = hs.load(dirpath + 'Heusler-IA57-2-15-June__default8__EDS.hspy', lazy=False)\n",
    "eds.set_signal_type(\"EDS_TEM\")\n",
    "\n",
    "dp = hs.load(dirpath + \"Heusler-IA57-2-15-June__default8.hspy\", lazy=False)\n",
    "dp.change_dtype(\"float32\")\n",
    "dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.plot(norm='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eds.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "Binning is mostly for computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4D-STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signal2D, title: , dimensions: (150, 50|128, 128)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_rebin = dp.rebin(scale=(1,1,2,2)) \n",
    "dp_rebin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_rebin_shape = dp_rebin.data.shape[2]\n",
    "dp_rebin_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation thresholding \n",
    "\n",
    "The within-feature standard deviation of each pixel in reciprocal space is calculated across all observations in the log-scaled 4D-STEM dataset resulting in a standard deviation map in reciprocal space. \n",
    "\n",
    "Log-scaling has to be applied to equalize standard deviations between weaker\n",
    "and stronger reflections.\n",
    "\n",
    "A central spot mask is applied next as it often contains significant variations that are uncorrelated to structure. \n",
    "\n",
    "A percentile threshold value is then defined, of which pixels corresponding to standard deviations above the threshold are chosen as features in a 2D array.\n",
    "\n",
    "In general, there is relatively high stability in results using threshold values between 60-80%. Ideally you would want to explore higher percentile thresholds as we are primarily interested in reducing the sparsity/background features present in the 4D-STEM dataset as much as possible.\n",
    "\n",
    "This particular feature engineering technique works better with discrete Bragg spots, and a different form of feature engineering might be required for other systems - e.g. radial profile, annular average, cepstrum etc. It is recommended to try cepstrum next if SDT is not working well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_rebin.fold()\n",
    "dp_rebin.set_signal_type('diffraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_rebin.unfold()\n",
    "\n",
    "std_dev_dp_pixels = np.std(dp_rebin.data, axis=0)\n",
    "std_dev_dp_pixels = hs.signals.Signal2D(\n",
    "    std_dev_dp_pixels.reshape(dp_rebin_shape, dp_rebin_shape)\n",
    ") \n",
    "\n",
    "# Define finalized center coordinates and radius of the circular region of interest\n",
    "center_x, center_y = (62, 60)\n",
    "radius = 9\n",
    "\n",
    "# Create a grid of pixel coordinates\n",
    "x, y = np.meshgrid(\n",
    "    np.arange(std_dev_dp_pixels.data.shape[0]),\n",
    "    np.arange(std_dev_dp_pixels.data.shape[1]),\n",
    ")\n",
    "\n",
    "# Calculate the distance of each pixel to the center of the circle\n",
    "distances = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)\n",
    "\n",
    "# Create a boolean mask where True values correspond to pixels within the circle\n",
    "mask = distances <= radius\n",
    "\n",
    "# Apply the mask to the image\n",
    "std_dev_dp_pixels_masked_central = np.copy(std_dev_dp_pixels.data)\n",
    "std_dev_dp_pixels_masked_central[mask] = 0\n",
    "\n",
    "\n",
    "std_dev_dp_pixels_masked_central = hs.signals.Signal2D(std_dev_dp_pixels_masked_central)\n",
    "std_dev_dp_pixels_masked_central.plot(norm='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_thres = 0.4\n",
    "\n",
    "matrix = std_dev_dp_pixels_masked_central.data\n",
    "\n",
    "n = int(np.ceil(matrix.size * final_thres))\n",
    "# n=1024\n",
    "\n",
    "# Flatten the matrix into a 1D array and sort it in descending order\n",
    "sorted_array = np.sort(matrix.flatten())[::-1]\n",
    "\n",
    "# Extract the highest n values from the sorted array\n",
    "top_n = sorted_array[:n]\n",
    "\n",
    "# Reshape the top_n array back into a 2D matrix\n",
    "# Create a mask for the top n values\n",
    "mask = np.isin(matrix, top_n)\n",
    "\n",
    "# Set all values not in the top n to zero\n",
    "result = matrix * mask\n",
    "\n",
    "\n",
    "result = hs.signals.Signal2D(result)\n",
    "result.plot(norm='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_rebin.fold()\n",
    "dp_rebin.unfold_navigation_space()\n",
    "\n",
    "extracted_pixels_list = []\n",
    "\n",
    "for d in dp_rebin.data:\n",
    "    extracted_pixels = d[np.where(mask==1)]\n",
    "    extracted_pixels_list.append(extracted_pixels)\n",
    "\n",
    "extracted_pixels_array = np.array(extracted_pixels_list)\n",
    "extracted_pixels_array[extracted_pixels_array < 0] = 0\n",
    "extracted_pixels_array_scaled = np.log(extracted_pixels_array, where=extracted_pixels_array != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataset after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.signals.Signal1D(extracted_pixels_array_scaled.reshape(50,150,-1)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check per-channel variance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eds.unfold()\n",
    "eds_var = np.var(eds.data, axis=0)\n",
    "hs.signals.Signal1D(eds_var).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract channels with channel variance greater than a certain threshold.\n",
    "\n",
    "Threshold is dependent on the dataset's background variance based on the plot above.\n",
    "\n",
    "In addition, certain known physical artefacts such as Cu peaks from the TEM grids can be manually excluded even if they represent high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 758)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eds.unfold()\n",
    "extracted_channel_list = []\n",
    "\n",
    "for spect in eds.data:\n",
    "    extracted_channel = spect[np.where(eds_var>0.012)]\n",
    "    extracted_channel_list.append(extracted_channel)\n",
    "\n",
    "extracted_channel_array = np.array(extracted_channel_list)\n",
    "extracted_channel_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eds.unfold()\n",
    "extracted_channel_array_scaled = np.log1p(extracted_channel_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 758)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_channel_array_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataset after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.signals.Signal1D(extracted_channel_array_scaled.reshape(50,150,-1)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform signal merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling of EDS\n",
    "\n",
    "The number of DP pixel features greatly outweigh the number of STEM-EDS\n",
    "channel features. \n",
    "\n",
    "In addition, the majority of DP pixel features contain relatively high\n",
    "intensity magnitudes, compared to the counts in the STEM-EDS channels.\n",
    "\n",
    "This illustrates the non-trivial problem of the fusion of blocks of data that have different scales, sizes and underlying dimensionalities.\n",
    "\n",
    "As such, a scaling factor is required to bias the STEM-EDS dataset further.\n",
    "\n",
    "It is recommended to do a sensitivity analysis as required, although a general rule of thumb that worked well between the systems tested is to scale the STEM-EDS dataset such that the maximum value is twice that of the 4D-STEM dataset. \n",
    "\n",
    "In general, you would want to bias towards the EDS dataset, as DP intensity variations within the same spots can lead to clustering based on tilts/thickness effects. The inclusion of weighted STEM-EDS suppresses these effects. \n",
    "\n",
    "The `np.exp` function is purely to help explore a wider range of values in a more efficient manner.\n",
    "\n",
    "It is important to include the inter-dataset scaler (RobustScaler) used in the later workflow as part of the sensitivity analysis as that directly affects the values of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 758), (7500, 6554))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_channel_array_scaled.shape, extracted_pixels_array_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.750880660985626"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling_factor = 2.4\n",
    "\n",
    "np.max(RobustScaler(with_centering=True).fit_transform(extracted_channel_array_scaled*np.exp(scaling_factor))) # max value is twice that of DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.307353"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(RobustScaler(with_centering=True).fit_transform(extracted_pixels_array_scaled*np.exp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = np.concatenate((extracted_pixels_array_scaled, extracted_channel_array_scaled*np.exp(scaling_factor)), axis=1)\n",
    "combined_data = hs.signals.Signal1D(combined_data.reshape(50,150,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform dimension reduction and clustering on merged datasets\n",
    "\n",
    "Hyperparameter understanding from official documentation:\n",
    "\n",
    "https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "\n",
    "https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "\n",
    "\n",
    "UMAP:\n",
    "\n",
    "`min_distance=0`, `densmap=True` and `n_neighbors=10` encourages compact and well-separated clusters and should generally not require changing based on previous experience. In general, higher values of `min_distance` and `n_neighbors` would encourage a relatively more continuous distribution of points in latent space as compared to compact and well-separated clusters. This may be necessary to change when considering more continuous changes such as diffusion processes.\n",
    "\n",
    "`n_components` generally produces stable cluster results across a wide range of components, and should not require changing even if the system complexity increases. This is unlike linear dimension reduction techniques, where additional components can drastically alter the microstructure component resulting from the linear addition of an entirely new phase. \n",
    "\n",
    "`metric` the distance metric used to compute the latent space. A more general starting metric to try would be `eucldiean`. `chebyshev` identifies the maximally dissimilar feature between two points and uses that to compute the distance. `euclidean` averages dissimilarity in all features between two points.\n",
    "\n",
    "HDBSCAN:\n",
    "\n",
    "`min_cluster_size` this was shown to be a relatively intuitive parameter that relates to the smallest microstrucrture feature present in the scan in terms of the number of pixels, which can be achieved just through the inverse 4D-STEM plot using Hyperspy `dp.T.plot()` or the HAADF associated with the STEM-EDS dataset.\n",
    "\n",
    "`min_samples` is not an important parameter in the context of soft implementation of HDBSCAN, which models points as part of multiple clusters instead of modelling the probability that a point belongs to a hard cluster label. The parameter controls how conservative it is in assigning points to clusters vs outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "combined_data.unfold()\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('scale', RobustScaler(with_centering=True)),\n",
    "        ('reduce_dims', umap.UMAP(densmap=True, n_neighbors=10, n_components=3, min_dist=0, metric='euclidean')),\n",
    "        ])\n",
    "\n",
    "reduced_data = pipe.fit_transform(combined_data.data)\n",
    "\n",
    "clust = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=1, prediction_data=True)\n",
    "\n",
    "clust.fit(reduced_data)\n",
    "\n",
    "x,y = 50,150\n",
    "\n",
    "memberships_all_soft = hdbscan.all_points_membership_vectors(clust)\n",
    "memberships_highest_soft = np.array([np.max(x) for x in memberships_all_soft])\n",
    "memberships_highest_soft_spatial = memberships_highest_soft.reshape(x, y)\n",
    "labels_highest_soft = np.array([np.argmax(x) for x in memberships_all_soft])\n",
    "labels_highest_soft_spatial = labels_highest_soft.reshape(x, y)\n",
    "no_cluster_soft = len(set(labels_highest_soft))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize cluster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_map(labels_highest_soft_spatial,no_cluster_soft) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_mean_analysis(include_eds_spectra=True)\n",
    "# Optional parameters allow plotting for associated EDS spectra, the per cluster standard deviations\n",
    "# The standard deviation maps in particular are helpful in assessing whether the clusters produced are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check stability of results\n",
    "\n",
    "The supplementary materials include a novel stability metric that identifies the stability of cluster results between different runs by producing a cluster stability map. For exploratory analysis, just re-running the workflow with the same parameters will give a good idea of microstructure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th run completed\n",
      "1th run completed\n",
      "2th run completed\n",
      "3th run completed\n",
      "4th run completed\n"
     ]
    }
   ],
   "source": [
    "combined_data.unfold()\n",
    "\n",
    "for i in range(5):\n",
    "        pipe = Pipeline([\n",
    "                ('scale', RobustScaler(with_centering=True)),\n",
    "                ('reduce_dims', umap.UMAP(densmap=True, n_neighbors=10, n_components=3, min_dist=0, metric='chebyshev')),\n",
    "                ])\n",
    "\n",
    "        reduced_data = pipe.fit_transform(combined_data.data)\n",
    "        clust = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=10, prediction_data=True)\n",
    "        clust.fit(reduced_data)\n",
    "\n",
    "        x,y = 50,150\n",
    "\n",
    "        memberships_all_soft = hdbscan.all_points_membership_vectors(clust)\n",
    "        memberships_highest_soft = np.array([np.max(x) for x in memberships_all_soft])\n",
    "        memberships_highest_soft_spatial = memberships_highest_soft.reshape(x, y)\n",
    "        labels_highest_soft = np.array([np.argmax(x) for x in memberships_all_soft])\n",
    "        labels_highest_soft_spatial = labels_highest_soft.reshape(x, y)\n",
    "        no_cluster_soft = len(set(labels_highest_soft))\n",
    "\n",
    "        plot_label_map(labels_highest_soft_spatial,no_cluster_soft) \n",
    "        \n",
    "        plt.title(f'{i}th run')\n",
    "        plt.savefig(f'{i}th run')\n",
    "        print(f'{i}th run completed')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed random seed to directly replicate results in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\datascience_3.9\\lib\\site-packages\\umap\\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "combined_data.unfold()\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('scale', RobustScaler(with_centering=True)),\n",
    "        ('reduce_dims', umap.UMAP(densmap=True, n_neighbors=10, n_components=3, min_dist=0, metric='chebyshev', random_state=10)),\n",
    "        ]) # random_state=10 is ok\n",
    "\n",
    "reduced_data = pipe.fit_transform(combined_data.data)\n",
    "clust = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=10, prediction_data=True)\n",
    "clust.fit(reduced_data)\n",
    "\n",
    "x,y = 50,150\n",
    "\n",
    "memberships_all_soft = hdbscan.all_points_membership_vectors(clust)\n",
    "memberships_highest_soft = np.array([np.max(x) for x in memberships_all_soft])\n",
    "memberships_highest_soft_spatial = memberships_highest_soft.reshape(x, y)\n",
    "labels_highest_soft = np.array([np.argmax(x) for x in memberships_all_soft])\n",
    "labels_highest_soft_spatial = labels_highest_soft.reshape(x, y)\n",
    "no_cluster_soft = len(set(labels_highest_soft))\n",
    "\n",
    "plot_label_map(labels_highest_soft_spatial,no_cluster_soft) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
