{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "%matplotlib qt5\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palette_for_label_map(no_cluster, unclassified=False, mask=False):\n",
    "    \"\"\"\n",
    "    no_cluster = number of real clusters\n",
    "    unclassified = no_cluster + 1\n",
    "    mask = no_cluster + 2\n",
    "\n",
    "    Returns palette, CustomCmap\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib\n",
    "\n",
    "    # Extract colors from tab20 and tab20b\n",
    "    tab20_colors = [plt.cm.tab20(i) for i in range(20)]\n",
    "    tab20b_colors = [plt.cm.tab20b(i) for i in range(20)]\n",
    "    combined_colors = tab20_colors + tab20b_colors\n",
    "\n",
    "    custom_palette = combined_colors[:no_cluster]\n",
    "\n",
    "    if unclassified == True:\n",
    "        custom_palette.insert(0, \"black\")  # UNCLASSIFIED\n",
    "    if mask == True:\n",
    "        custom_palette.insert(0, \"white\")  # MASKED OUT\n",
    "\n",
    "    CustomCmap = matplotlib.colors.ListedColormap(custom_palette)\n",
    "    palette = sns.color_palette(palette=custom_palette)\n",
    "    return (palette, CustomCmap)\n",
    "\n",
    "def include_scalebar(dp):\n",
    "    from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "    dx = dp.axes_manager[0].scale\n",
    "    scalebar = ScaleBar(\n",
    "        dx,\n",
    "        \"nm\",\n",
    "        length_fraction=0.25,\n",
    "        width_fraction=0.015,\n",
    "        location=\"lower left\",\n",
    "        frameon=False,\n",
    "        color=\"w\",\n",
    "        scale_loc=\"top\",\n",
    "        border_pad=0.5,\n",
    "    )\n",
    "    plt.gca().add_artist(scalebar)\n",
    "\n",
    "\n",
    "def plot_label_map(labels_highest_soft_spatial, no_cluster_soft, unclassified=False, scalebar=False):\n",
    "    palette, CustomCmap = palette_for_label_map(\n",
    "        no_cluster_soft, unclassified=unclassified, mask=False\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(labels_highest_soft_spatial, cmap=CustomCmap)\n",
    "\n",
    "    if scalebar == True:\n",
    "        include_scalebar(dp)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_labels\\0_control_labels.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_labels\\1_std_dev_thres_labels.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_labels\\2_bina_labels.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_labels\\3_rv_labels.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_labels\\4_aa_labels.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_labels\\5_ift_labels.npy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_clustering_results(folder_path):\n",
    "    \"\"\"\n",
    "    Loads clustering results from .npy files in a folder and creates an array.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .npy clustering result files.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array where each row corresponds to clustering labels from one file.\n",
    "    \"\"\"\n",
    "    clustering_results = []\n",
    "\n",
    "    # Loop through all .npy files in the folder\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".npy\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print(file_path)\n",
    "            labels = np.load(file_path)\n",
    "            if labels.ndim > 1:  # Flatten if labels are in a column/row\n",
    "                labels = labels.flatten()\n",
    "            clustering_results.append(labels)\n",
    "    \n",
    "    # Convert to a NumPy array\n",
    "    clustering_results = np.array(clustering_results)\n",
    "    return np.array(clustering_results)\n",
    "\n",
    "labels_path = \"C:\\\\Users\\Zhi Quan\\\\Dropbox (Personal)\\\\Jupyter backup\\TRISO\\\\2025\\data\\\\feature_engi_labels\\\\\"\n",
    "labels = load_clustering_results(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_memberships\\0_control_memberships.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_memberships\\1_std_dev_thres_memberships.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_memberships\\2_bina_memberships.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_memberships\\3_rv_memberships.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_memberships\\4_aa_memberships.npy\n",
      "C:\\Users\\Zhi Quan\\Dropbox (Personal)\\Jupyter backup\\TRISO\\2025\\data\\feature_engi_memberships\\5_ift_memberships.npy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_membership_weights(file_paths, file_format='npy'):\n",
    "    \"\"\"\n",
    "    Load membership weights from files into a list of numpy arrays.\n",
    "    \n",
    "    Parameters:\n",
    "        file_paths (str or list of str): Path(s) to membership weight files or a directory containing them.\n",
    "        file_format (str): Format of the files ('npy', 'csv', or 'txt'). Default is 'npy'.\n",
    "        \n",
    "    Returns:\n",
    "        list of np.ndarray: List of membership weight matrices (n_samples x n_clusters).\n",
    "    \"\"\"\n",
    "    # If a directory is provided, get all files with the given format\n",
    "    if isinstance(file_paths, str) and os.path.isdir(file_paths):\n",
    "        file_paths = sorted([\n",
    "            os.path.join(file_paths, f)\n",
    "            for f in os.listdir(file_paths)\n",
    "            if f.endswith(f\".{file_format}\")\n",
    "        ])\n",
    "    elif isinstance(file_paths, str):\n",
    "        file_paths = [file_paths]\n",
    "    \n",
    "    # Check for valid file paths\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No valid file paths provided or found in the directory.\")\n",
    "    \n",
    "    # Load weights from each file\n",
    "    weight_matrices = []\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise ValueError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        if file_format == 'npy':\n",
    "            print(file_path)\n",
    "            weight_matrix = np.load(file_path)\n",
    "        elif file_format in ['csv', 'txt']:\n",
    "            weight_matrix = np.loadtxt(file_path, delimiter=',')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "        \n",
    "        weight_matrices.append(weight_matrix)\n",
    "    \n",
    "    return weight_matrices\n",
    "\n",
    "\n",
    "memberships_path = \"C:\\\\Users\\Zhi Quan\\\\Dropbox (Personal)\\\\Jupyter backup\\TRISO\\\\2025\\data\\\\feature_engi_memberships\\\\\"\n",
    "clusters_all_memberships_list = load_membership_weights(memberships_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 25600)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_single_membership_list = []\n",
    "\n",
    "for cluster in clusters_all_memberships_list:\n",
    "    single_membership = np.array([np.max(x) for x in cluster])\n",
    "    clusters_single_membership_list.append(single_membership)\n",
    "\n",
    "clusters_single_membership_list = np.array(clusters_single_membership_list)\n",
    "clusters_single_membership_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "clusters_single_membership_list = Normalizer('l1').fit_transform(clusters_single_membership_list.T).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft label implementation (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def soft_label_consensus(cluster_runs_list, label_memberships):\n",
    "    \"\"\"\n",
    "    Compute a consensus distance matrix based on clustering labels and associated membership values,\n",
    "    considering only the clustering run with the maximum pairwise weight for each label.\n",
    "\n",
    "    Args:\n",
    "        cluster_runs_list (list of np.ndarray): List of clustering label arrays (one array per clustering).\n",
    "        label_memberships (list of np.ndarray): List of membership value arrays corresponding to cluster labels.\n",
    "                                                Each array matches the length of the corresponding label array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A consensus distance matrix (1 - normalized co-occurrence).\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n_samples = len(cluster_runs_list[0])\n",
    "    \n",
    "    # Initialize matrices\n",
    "    co_occurrence_matrix = np.zeros((n_samples, n_samples), dtype=float)\n",
    "    max_pairwise_weights = np.zeros((n_samples, n_samples), dtype=float)\n",
    "    \n",
    "    # Process each clustering run\n",
    "    for clustering, memberships in zip(cluster_runs_list, label_memberships):\n",
    "        # Unique cluster labels and indices\n",
    "        unique_labels = np.unique(clustering)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            # Find indices of points in the current cluster\n",
    "            cluster_indices = np.where(clustering == label)[0]\n",
    "\n",
    "            # Skip small clusters\n",
    "            if len(cluster_indices) <= 1:\n",
    "                continue\n",
    "\n",
    "            cluster_memberships = memberships[cluster_indices]\n",
    "        \n",
    "            # Compute pairwise weights (vectorized outer product)\n",
    "            pairwise_weights = np.outer(cluster_memberships, cluster_memberships)\n",
    "\n",
    "            \n",
    "            # Update co-occurrence and max-pairwise weights using broadcasting\n",
    "            idx_i, idx_j = np.meshgrid(cluster_indices, cluster_indices, indexing=\"ij\")\n",
    "            mask = pairwise_weights >= max_pairwise_weights[idx_i, idx_j]\n",
    "\n",
    "            # print(\"Max weights before update:\", max_pairwise_weights[idx_i[mask], idx_j[mask]])\n",
    "            max_pairwise_weights[idx_i[mask], idx_j[mask]] = pairwise_weights[mask]\n",
    "\n",
    "    max_pairwise_weights = MinMaxScaler().fit_transform(max_pairwise_weights)\n",
    "    \n",
    "    # Convert similarity to distance\n",
    "    distance_matrix = 1 - max_pairwise_weights\n",
    "    \n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = soft_label_consensus(labels, clusters_single_membership_list_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "HDBSCAN does not work well with the distance matrix created. Use agglo clustering instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply hierarchical clustering to the co-occurrence matrix\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    distance_threshold=0.8,\n",
    "    n_clusters=None,\n",
    "    metric='precomputed',\n",
    "    linkage='average'\n",
    ")\n",
    "consensus_labels = hierarchical.fit_predict(distance_matrix)  # Dissimilarity for agglomerative clustering\n",
    "\n",
    "print(np.unique(consensus_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('consensus_labels.npy', consensus_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(consensus_labels.reshape(160,160), cmap='tab20')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3 weighted (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_label_consensus_with_subclusters(cluster_runs_list, label_memberships, penalization_weight=0.5):\n",
    "    \"\"\"\n",
    "    Compute a consensus distance matrix to encourage finer sub-clusters,\n",
    "    balancing co-occurrence with variability across clustering runs.\n",
    "\n",
    "    Args:\n",
    "        cluster_runs_list (list of np.ndarray): Clustering label arrays from different runs.\n",
    "        label_memberships (list of np.ndarray): Membership value arrays corresponding to the label arrays.\n",
    "        penalization_weight (float): Threshold for penalizing overly broad consensus. variability threshold = 0 results in complete disabling of the variability penalty. The function behaves like the original implementation, focusing purely on pairwise co-occurrence without penalizing inconsistent clustering.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A consensus distance matrix (1 - normalized co-occurrence).\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    n_samples = len(cluster_runs_list[0])\n",
    "    \n",
    "    # Initialize matrices\n",
    "    co_occurrence_matrix = np.zeros((n_samples, n_samples), dtype=float)\n",
    "    run_contribution_matrix = np.zeros((n_samples, n_samples), dtype=float)\n",
    "\n",
    "    # Process each clustering run\n",
    "    for clustering, memberships in zip(cluster_runs_list, label_memberships):\n",
    "        unique_labels = np.unique(clustering)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            cluster_indices = np.where(clustering == label)[0]\n",
    "            if len(cluster_indices) <= 1:\n",
    "                continue\n",
    "            \n",
    "            cluster_memberships = memberships[cluster_indices]\n",
    "            pairwise_weights = np.outer(cluster_memberships, cluster_memberships)\n",
    "            \n",
    "            idx_i, idx_j = np.meshgrid(cluster_indices, cluster_indices, indexing=\"ij\")\n",
    "            co_occurrence_matrix[idx_i, idx_j] += pairwise_weights\n",
    "            run_contribution_matrix[idx_i, idx_j] += 1\n",
    "\n",
    "    # Normalize co-occurrence by the number of cluster runs\n",
    "    co_occurrence_matrix /= len(cluster_runs_list)\n",
    "    run_contribution_matrix /= len(cluster_runs_list)\n",
    "\n",
    "    # Apply variability penalty\n",
    "    penalized_similarity_matrix  = co_occurrence_matrix * (1 - np.abs(run_contribution_matrix - 1) * penalization_weight)\n",
    "\n",
    "    # Normalize similarity matrix\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_similarity = scaler.fit_transform(penalized_similarity_matrix)\n",
    "\n",
    "    # Convert similarity to distance\n",
    "    distance_matrix = 1 - normalized_similarity\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 : 13\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import hdbscan\n",
    "\n",
    "# # Delete unnecessary variables to free up memory\n",
    "# del distance_matrix_sub\n",
    "# del clust\n",
    "# gc.collect()\n",
    "\n",
    "distance_matrix_sub = soft_label_consensus_with_subclusters(labels, clusters_single_membership_list, penalization_weight=value)\n",
    "\n",
    "clust = hdbscan.HDBSCAN(min_cluster_size=250, min_samples=25, metric='precomputed')\n",
    "clust.fit(distance_matrix_sub)\n",
    "\n",
    "print(f'{value} : {len(np.unique(clust.labels_))}')\n",
    "plot_label_map(clust.labels_.reshape(160,160), no_cluster_soft=len(np.unique(clust.labels_))-1, unclassified=True)\n",
    "\n",
    "# plt.savefig(f'{value}_weight_label_map_hdbscan.png')\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x20299901fd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(clust.probabilities_.reshape(160,160))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
